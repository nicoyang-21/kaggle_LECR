{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OdPXWBnoVMP3"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p /root/.kaggle\n",
    "# !cp kaggle.json /root/.kaggle/\n",
    "# !chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jMXVFzxXVWya"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading learning-equality-curriculum-recommendations.zip to /media/will/data/LECR\n",
      " 99%|███████████████████████████████████████▋| 252M/254M [00:19<00:00, 18.8MB/s]\n",
      "100%|████████████████████████████████████████| 254M/254M [00:19<00:00, 13.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "#!kaggle competitions download -c learning-equality-curriculum-recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Y7BduhbrV7mm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  learning-equality-curriculum-recommendations.zip\n",
      "  inflating: content.csv             \n",
      "  inflating: correlations.csv        \n",
      "  inflating: sample_submission.csv   \n",
      "  inflating: topics.csv              \n"
     ]
    }
   ],
   "source": [
    "#!unzip learning-equality-curriculum-recommendations.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMNKoFlzTA2i",
    "outputId": "b0011368-3012-4f5e-ddd7-ef53e9fd322c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.tuna.tsinghua.edu.cn'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "Collecting sentencepiece\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.tuna.tsinghua.edu.cn'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/0e/7e/a69d054029c7c0470e490b3265bbd1497df9492599b1820b9d5be2c60444/sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers -q\n",
    "# !pip install multiprocesspandas -q\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyfn8McqUEBQ"
   },
   "source": [
    "## CV Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XfVCwUiETLWX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from multiprocesspandas import applyparallel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_x264mcXT-5R"
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KpzDtQBUUCd1"
   },
   "outputs": [],
   "source": [
    "topic_df = pd.read_csv('topics.csv')\n",
    "content_df = pd.read_csv('content.csv')\n",
    "corr_df = pd.read_csv('correlations.csv')\n",
    "# topic_df = topic_df.rename(columns={'id': 'topic_id'}).merge(corr_df)\n",
    "topic_df_non_source = topic_df[topic_df['category']!='source'].reset_index(drop=True)\n",
    "topic_df_non_source['stratify'] = topic_df_non_source['category'] + \\\n",
    "topic_df_non_source['language'] + topic_df_non_source['description'].apply(lambda x: str(isinstance(x, str))) + \\\n",
    "topic_df_non_source['has_content'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_lvM_JZuUnoT",
    "outputId": "ee34b1d5-16eb-426c-aa65-5af83917a603"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_split.py:909: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedGroupKFold(n_splits=N_SPLITS)\n",
    "folds = list(kf.split(topic_df_non_source, y=topic_df_non_source[\"stratify\"], groups=topic_df_non_source[\"channel\"]))\n",
    "topic_df_non_source['fold'] = -1\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "    topic_df_non_source.loc[val_idx, \"fold\"] = fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "O6YY6bJkUt5H"
   },
   "outputs": [],
   "source": [
    "fold_df =  topic_df.merge(topic_df_non_source[['id', 'fold']], on='id', how='left').reset_index(drop=True)[['id', 'fold']].fillna(-1).rename(columns={'id': 'topic_id'})\n",
    "fold_df['fold'] = fold_df['fold'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "e9L5F7uqWhpl"
   },
   "outputs": [],
   "source": [
    "corr_df['content_ids'] = corr_df['content_ids'].apply(lambda x:x.split())\n",
    "corr_df = corr_df.explode('content_ids').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "r94vw5-gXHoX"
   },
   "outputs": [],
   "source": [
    "topic_df = topic_df.fillna('')\n",
    "topic_df['topic_full_text'] =  topic_df['title'] + ' [SEP] ' + topic_df['description']\n",
    "topic_df = topic_df[['id', 'topic_full_text', 'language']]\n",
    "df = corr_df.merge(topic_df, left_on='topic_id', right_on='id', how='left')\n",
    "df = df[['topic_id','content_ids','topic_full_text','language']]\n",
    "df = df.rename(columns={'language':'topic_language'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DA20jcVaXkkP"
   },
   "outputs": [],
   "source": [
    "content_df = content_df.fillna('')\n",
    "content_df['content_full_text'] =  content_df['title'] + ' [SEP] ' + content_df['description'] + ' [SEP] ' + content_df['text']\n",
    "content_df = content_df[['id', 'content_full_text', 'language']]\n",
    "df = df.merge(content_df, left_on='content_ids', right_on='id', how='left')\n",
    "df = df.rename(columns={'language':'content_language'})\n",
    "df['label'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iO0Hdds1n7_-"
   },
   "source": [
    "## random sample according to language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Ae5YOFOh4AF",
    "outputId": "c8e5b398-9cac-4874-af8d-4dd212dff70e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61517/61517 [37:58<00:00, 27.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_full_text</th>\n",
       "      <th>content_full_text</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>259538</th>\n",
       "      <td>Закръгляване на десетични дроби [SEP] Научи ка...</td>\n",
       "      <td>Закръгляване на десетични дроби - предизвикате...</td>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46887</th>\n",
       "      <td>Формула за смяна на основата при логаритми [SE...</td>\n",
       "      <td>Използване на правилото за смяна на основата п...</td>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106289</th>\n",
       "      <td>Разпознаване на линейната функция [SEP] В този...</td>\n",
       "      <td>Разпознаване на линейни функции [SEP] Научи се...</td>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>Ъгли [SEP] Преговори знанията си за ъгли.</td>\n",
       "      <td>Упражнения с уравнения с допълващи се ъгли [SE...</td>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17812</th>\n",
       "      <td>Построяване на ъглополовящи прави и ъгли [SEP]...</td>\n",
       "      <td>Геометрични построения: перпендикулярна права ...</td>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74105</th>\n",
       "      <td>مِفْتاحُ القُلوبِ [SEP]</td>\n",
       "      <td>المفردات والتراكيب [SEP]  [SEP] صحّح الخطأ في ...</td>\n",
       "      <td>t_fffe811a6da9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209127</th>\n",
       "      <td>في وَداعِ تِلْميذ [SEP]</td>\n",
       "      <td>في وَداعِ تِلْميذ [SEP] رسالة إلكترونيّة تجمع ...</td>\n",
       "      <td>t_fffe811a6da9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37181</th>\n",
       "      <td>\"زها الحَديديَّةُ\" [SEP]</td>\n",
       "      <td>المفردات والتراكيب [SEP]  [SEP] ما ضدّ كلمة (ق...</td>\n",
       "      <td>t_fffe811a6da9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60160</th>\n",
       "      <td>النظريات الاقتصادية [SEP] شرح النظريات الاقتصا...</td>\n",
       "      <td>النظرية الكينزية - جون مينارد كينز | النظريات ...</td>\n",
       "      <td>t_fffe811a6da9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180378</th>\n",
       "      <td>يستقصي أثر تعديل البيانات في مقاييس النزعة الم...</td>\n",
       "      <td>Video [SEP] 5adefc3a6b9064043c6481be [SEP]</td>\n",
       "      <td>t_fffe811a6da9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307585 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          topic_full_text  \\\n",
       "259538  Закръгляване на десетични дроби [SEP] Научи ка...   \n",
       "46887   Формула за смяна на основата при логаритми [SE...   \n",
       "106289  Разпознаване на линейната функция [SEP] В този...   \n",
       "2689            Ъгли [SEP] Преговори знанията си за ъгли.   \n",
       "17812   Построяване на ъглополовящи прави и ъгли [SEP]...   \n",
       "...                                                   ...   \n",
       "74105                            مِفْتاحُ القُلوبِ [SEP]    \n",
       "209127                           في وَداعِ تِلْميذ [SEP]    \n",
       "37181                           \"زها الحَديديَّةُ\" [SEP]    \n",
       "60160   النظريات الاقتصادية [SEP] شرح النظريات الاقتصا...   \n",
       "180378  يستقصي أثر تعديل البيانات في مقاييس النزعة الم...   \n",
       "\n",
       "                                        content_full_text        topic_id  \\\n",
       "259538  Закръгляване на десетични дроби - предизвикате...  t_00004da3a1b2   \n",
       "46887   Използване на правилото за смяна на основата п...  t_00004da3a1b2   \n",
       "106289  Разпознаване на линейни функции [SEP] Научи се...  t_00004da3a1b2   \n",
       "2689    Упражнения с уравнения с допълващи се ъгли [SE...  t_00004da3a1b2   \n",
       "17812   Геометрични построения: перпендикулярна права ...  t_00004da3a1b2   \n",
       "...                                                   ...             ...   \n",
       "74105   المفردات والتراكيب [SEP]  [SEP] صحّح الخطأ في ...  t_fffe811a6da9   \n",
       "209127  في وَداعِ تِلْميذ [SEP] رسالة إلكترونيّة تجمع ...  t_fffe811a6da9   \n",
       "37181   المفردات والتراكيب [SEP]  [SEP] ما ضدّ كلمة (ق...  t_fffe811a6da9   \n",
       "60160   النظرية الكينزية - جون مينارد كينز | النظريات ...  t_fffe811a6da9   \n",
       "180378        Video [SEP] 5adefc3a6b9064043c6481be [SEP]   t_fffe811a6da9   \n",
       "\n",
       "        label  \n",
       "259538      0  \n",
       "46887       0  \n",
       "106289      0  \n",
       "2689        0  \n",
       "17812       0  \n",
       "...       ...  \n",
       "74105       0  \n",
       "209127      0  \n",
       "37181       0  \n",
       "60160       0  \n",
       "180378      0  \n",
       "\n",
       "[307585 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_df = []\n",
    "sample_n = 5\n",
    "\n",
    "def negative_smaple(x, candidates):\n",
    "    topic_language = x['topic_language'][0]\n",
    "    candidates = candidates[candidates['content_language'] == topic_language]\n",
    "\n",
    "    return candidates[['topic_full_text', 'content_full_text']].sample(n=sample_n)\n",
    "\n",
    "for topic_id in tqdm(df['topic_id'].unique()):\n",
    "    sub_df = df[df['topic_id'] == topic_id]\n",
    "    topic_language = sub_df['topic_language'].unique()[0]\n",
    "    candidates = df[df['content_language'] == topic_language]\n",
    "    sample_neg = candidates[['topic_full_text', 'content_full_text']].sample(n=sample_n)\n",
    "    sample_neg['topic_id'] = topic_id\n",
    "    sample_neg['label'] = 0\n",
    "    neg_df.append(sample_neg)\n",
    "neg_df = pd.concat(neg_df)\n",
    "neg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "NhAgbgLbq4SM"
   },
   "outputs": [],
   "source": [
    "df = df[['topic_id', 'topic_full_text', 'content_full_text', 'label']]\n",
    "df = pd.concat([df, neg_df])\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "SoS2ECSO7hfd"
   },
   "outputs": [],
   "source": [
    "df = df.merge(fold_df, left_on='topic_id', right_on='topic_id', how='left')\n",
    "df = df[['topic_full_text', 'content_full_text', 'label' ,'fold']]\n",
    "df = df[df['fold'].isin([0, 1, 2, 3, 4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "c5XnPHddrrA7"
   },
   "outputs": [],
   "source": [
    "df.to_csv('train_folds.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OP_LxmnFb4ry"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_folds.csv')\n",
    "df = df[df['fold'].isin([0, 1, 2, 3, 4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "flmIFIdcEwkZ",
    "outputId": "70deaf8e-bae0-4110-a9ad-27dbe45a21ab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_full_text</th>\n",
       "      <th>content_full_text</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100 સુધીનો સરવાળો [SEP] 37 અને 49 જેવી બે-અંકન...</td>\n",
       "      <td>સમૂહ બનાવ્યા વિના 2-અંકની સંખ્યા ઉમેરવી 2 [SEP...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100 સુધીનો સરવાળો [SEP] 37 અને 49 જેવી બે-અંકન...</td>\n",
       "      <td>સમૂહ બનાવીને ઉમેરવું  [SEP] સલ સ્થાન કિંમત વિશ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100 સુધીનો સરવાળો [SEP] 37 અને 49 જેવી બે-અંકન...</td>\n",
       "      <td>સ્થાનકિંમતના બ્લોકનો ઉપયોગ કરી 100 સુધીની સંખ્...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12. 20: Bird Reproduction [SEP]</td>\n",
       "      <td>12. 20: Bird Reproduction [SEP]  [SEP] Is this...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12. 20: Bird Reproduction [SEP]</td>\n",
       "      <td>Astounding Mating Dance Birds of Paradise -- H...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227620</th>\n",
       "      <td>Movimiento Armónico Simple (MAS) [SEP] Estudio...</td>\n",
       "      <td>Péndulos [SEP] Explicamos cómo podemos tratar ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227621</th>\n",
       "      <td>Módulo 1 [SEP] En este módulo, los estudiantes...</td>\n",
       "      <td>9.1.1_st._lucy's_home_for_girls_raised_by_wolv...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227622</th>\n",
       "      <td>Solución de problemas con distancia en el plan...</td>\n",
       "      <td>Puntos dentro, fuera o sobre un círculo [SEP] ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227623</th>\n",
       "      <td>Middle School [SEP]</td>\n",
       "      <td>Fuerzas y Movimiento: Fundamentos [SEP] Explor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227624</th>\n",
       "      <td>Moléculas importantes para la biología [SEP] H...</td>\n",
       "      <td>Elementos y átomos [SEP] Cómo se relacionan lo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227625 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          topic_full_text  \\\n",
       "0       100 સુધીનો સરવાળો [SEP] 37 અને 49 જેવી બે-અંકન...   \n",
       "1       100 સુધીનો સરવાળો [SEP] 37 અને 49 જેવી બે-અંકન...   \n",
       "2       100 સુધીનો સરવાળો [SEP] 37 અને 49 જેવી બે-અંકન...   \n",
       "3                        12. 20: Bird Reproduction [SEP]    \n",
       "4                        12. 20: Bird Reproduction [SEP]    \n",
       "...                                                   ...   \n",
       "227620  Movimiento Armónico Simple (MAS) [SEP] Estudio...   \n",
       "227621  Módulo 1 [SEP] En este módulo, los estudiantes...   \n",
       "227622  Solución de problemas con distancia en el plan...   \n",
       "227623                               Middle School [SEP]    \n",
       "227624  Moléculas importantes para la biología [SEP] H...   \n",
       "\n",
       "                                        content_full_text  label  fold  \n",
       "0       સમૂહ બનાવ્યા વિના 2-અંકની સંખ્યા ઉમેરવી 2 [SEP...      1     2  \n",
       "1       સમૂહ બનાવીને ઉમેરવું  [SEP] સલ સ્થાન કિંમત વિશ...      1     2  \n",
       "2       સ્થાનકિંમતના બ્લોકનો ઉપયોગ કરી 100 સુધીની સંખ્...      1     2  \n",
       "3       12. 20: Bird Reproduction [SEP]  [SEP] Is this...      1     3  \n",
       "4       Astounding Mating Dance Birds of Paradise -- H...      1     3  \n",
       "...                                                   ...    ...   ...  \n",
       "227620  Péndulos [SEP] Explicamos cómo podemos tratar ...      0     0  \n",
       "227621  9.1.1_st._lucy's_home_for_girls_raised_by_wolv...      0     0  \n",
       "227622  Puntos dentro, fuera o sobre un círculo [SEP] ...      0     0  \n",
       "227623  Fuerzas y Movimiento: Fundamentos [SEP] Explor...      0     0  \n",
       "227624  Elementos y átomos [SEP] Cómo se relacionan lo...      0     0  \n",
       "\n",
       "[227625 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jsi4Pr8wWqM"
   },
   "source": [
    "## create CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Vu2P55dhwp5G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "from transformers import BertTokenizer,AutoModel,AdamW,AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "import random\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CbJxrkWxxWti"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    input_path = '/media/will/data/LECR'\n",
    "    model_path = 'microsoft/mdeberta-v3-base' \n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5  # 1.5\n",
    "    num_warmup_steps = 0\n",
    "    max_input_length = 124\n",
    "    epochs = 5  # 5\n",
    "    encoder_lr = 20e-6\n",
    "    decoder_lr = 1e-3\n",
    "    min_lr = 0.5e-6\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    weight_decay = 0\n",
    "    num_fold = 5\n",
    "    batch_size = 32\n",
    "    seed = 1006\n",
    "    OUTPUT_DIR = '/media/will/data/LECR'\n",
    "    num_workers = 2\n",
    "    device='cuda'\n",
    "    print_freq = 100\n",
    "    apex=False\n",
    "    start_awp_epoch = 2 # 开始AWP epoch\n",
    "    adv_lr = 1e-5 # AWP学习率\n",
    "    adv_eps = 1e-3 # AWP epsilon\n",
    "    adv_step = 1 # AWP step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "safR828DxavN"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "j1-oQIqpwalb"
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer):\n",
    "        self.topic = df['topic_full_text'].values\n",
    "        self.content = df['content_full_text'].values\n",
    "        self.label = df['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "    def __len__(self):\n",
    "        return len(self.topic)\n",
    "    def __getitem__(self, item):\n",
    "        topic = self.topic[item].replace('[SEP]', self.sep_token)\n",
    "        content = self.content[item].replace('[SEP]', self.sep_token)\n",
    "        label = int(self.label[item])\n",
    "\n",
    "        \n",
    "        inputs_topic = self.tokenizer(topic, truncation=True, max_length=CFG.max_input_length, padding='max_length')\n",
    "        inputs_content = self.tokenizer(content, truncation=True, max_length=CFG.max_input_length, padding='max_length')\n",
    "        return torch.as_tensor(inputs_topic['input_ids'], dtype=torch.long), \\\n",
    "            torch.as_tensor(inputs_topic['attention_mask'], dtype=torch.long), \\\n",
    "            torch.as_tensor(inputs_content['input_ids'], dtype=torch.long), \\\n",
    "            torch.as_tensor(inputs_content['attention_mask'], dtype=torch.long), \\\n",
    "            torch.as_tensor(label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnCXK1kYzPCo"
   },
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "i0w2WY_OzLeC"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "class Custom_Bert_Simple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path)\n",
    "        self.config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        self.linear = nn.Linear(self.config.hidden_size*3, 1)\n",
    "\n",
    "    def forward(self,\n",
    "        topic_input_ids,\n",
    "        content_input_ids,\n",
    "        topic_attention_mask=None,\n",
    "        content_attention_mask=None, \n",
    "        labels=None):\n",
    "        topic_output = self.base(input_ids=topic_input_ids,attention_mask=topic_attention_mask)\n",
    "        topic_output = topic_output.last_hidden_state\n",
    "        topic_output = torch.mean(topic_output, dim=1)\n",
    "\n",
    "        content_output = self.base(input_ids=content_input_ids,attention_mask=content_attention_mask)\n",
    "        content_output = content_output.last_hidden_state\n",
    "        content_output = torch.mean(content_output, dim=1)\n",
    "\n",
    "        diff = torch.abs(topic_output-content_output)\n",
    "\n",
    "        sentence_embedding = torch.cat([topic_output, content_output, diff], 1)\n",
    "\n",
    "        output = self.linear(sentence_embedding)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.binary_cross_entropy_with_logits(output.view(-1), labels.view(-1))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xB4GaI9fDzt1"
   },
   "source": [
    "## build logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "R3SPEdriD2lE"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6VVH3jSEYie",
    "outputId": "b0d4793c-ef1e-4623-b4f7-9a4f6ce628cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===============lr_2e-05===============\n",
      "===============seed_1006===============\n",
      "===============total_epochs_5===============\n",
      "===============num_warmup_steps_0===============\n"
     ]
    }
   ],
   "source": [
    "def get_logger(filename=CFG.OUTPUT_DIR+ 'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "LOGGER.info('===============lr_{}==============='.format(CFG.encoder_lr))\n",
    "LOGGER.info('===============seed_{}==============='.format(CFG.seed))\n",
    "LOGGER.info('===============total_epochs_{}==============='.format(CFG.epochs))\n",
    "LOGGER.info('===============num_warmup_steps_{}==============='.format(CFG.num_warmup_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQDtzCGV5S0B"
   },
   "source": [
    "## build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "S6Pg-_675VB2"
   },
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = [i.to(device) for i in batch]\n",
    "        topic_input_ids, topic_attention_mask, content_input_ids, content_attention_mask, label = batch\n",
    "        batch_size = label.size(0)\n",
    "        loss = model(topic_input_ids, content_input_ids, topic_attention_mask, content_attention_mask, label)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 500)\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    start = end = time.time()\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        label = batch[2].to(device)\n",
    "        mask = batch[1].to(device)\n",
    "        input_ids = batch[0].to(device)\n",
    "        batch_size = label.size(0)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, mask, labels=label)\n",
    "        loss = output.loss\n",
    "        y_preds = output.logits.argmax(dim=-1)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "        labels.append(label.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    #print(predictions)\n",
    "    return losses.avg, predictions, labels\n",
    "\n",
    "def train_loop(fold, model, train_dataset, valid_dataset):\n",
    "    LOGGER.info(f\"========== training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    #model = Custom_Bert_Simple()\n",
    "    #model = AutoModelForSequenceClassification.from_pretrained(CFG.model_path, num_labels=1)\n",
    "    model.to(CFG.device)\n",
    "\n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    def get_optimizer(model):\n",
    "\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                'lr': CFG.encoder_lr, 'weight_decay': CFG.weight_decay},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                'lr': CFG.encoder_lr, 'weight_decay': 0.0}\n",
    "            \n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_parameters, lr = CFG.encoder_lr, eps = CFG.eps, betas = CFG.betas)\n",
    "        return optimizer\n",
    "\n",
    "    \n",
    "    optimizer = get_optimizer(model)\n",
    "\n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        cfg.num_warmup_steps = cfg.num_warmup_steps * num_train_steps\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps,\n",
    "                num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "\n",
    "    num_train_steps = int(len(train_dataset) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    # criterion = torch.nn.CrossEntropyLoss(ignore_index=- 1)\n",
    "\n",
    "    # criterion = LabelSmoothingLoss()\n",
    "    best_score = float('inf')\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        #avg_loss = train_fn_awp(train_loader, model, optimizer, epoch, scheduler, CFG.device)\n",
    "        \n",
    "        avg_loss = train_fn(train_loader, model, optimizer, epoch, scheduler, CFG.device)\n",
    "        # eval\n",
    "        #avg_val_loss, predictions, valid_labels = valid_fn(valid_loader, model, CFG.device)\n",
    "\n",
    "        # scoring\n",
    "        #score = get_score(predictions, valid_labels)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(\n",
    "            f'Epoch {epoch + 1} - avg_train_loss: {avg_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        #LOGGER.info(f'Epoch {epoch + 1} - Score: {score:.4f}')\n",
    "\n",
    "\n",
    "        if best_score > avg_loss:\n",
    "            best_score = avg_loss\n",
    "            #best_predictions = predictions\n",
    "            LOGGER.info(f'Epoch {epoch + 1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save(model.state_dict(),\n",
    "                       CFG.OUTPUT_DIR + \"{}_best{}.pth\".format(CFG.model_path.replace('/', '_'),fold))\n",
    "\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    del scheduler, optimizer, model\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kazL85iWEb5W",
    "outputId": "5953d979-3e8b-4911-8876-834e16ceb357"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/mdeberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "========== training ==========\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/5462] Elapsed 0m 1s (remain 110m 34s) Loss: 0.7121(0.7121) Grad: 1.5052  LR: 0.00002000  \n",
      "Epoch: [1][100/5462] Elapsed 0m 43s (remain 38m 14s) Loss: 0.6459(0.6877) Grad: 3.1602  LR: 0.00002000  \n",
      "Epoch: [1][200/5462] Elapsed 1m 25s (remain 37m 16s) Loss: 0.6889(0.6837) Grad: 1.9917  LR: 0.00002000  \n",
      "Epoch: [1][300/5462] Elapsed 2m 7s (remain 36m 30s) Loss: 0.6667(0.6811) Grad: 1.6751  LR: 0.00001999  \n",
      "Epoch: [1][400/5462] Elapsed 2m 50s (remain 35m 46s) Loss: 0.6724(0.6783) Grad: 3.2433  LR: 0.00001999  \n",
      "Epoch: [1][500/5462] Elapsed 3m 32s (remain 35m 3s) Loss: 0.7144(0.6741) Grad: 4.5851  LR: 0.00001998  \n",
      "Epoch: [1][600/5462] Elapsed 4m 14s (remain 34m 21s) Loss: 0.6096(0.6719) Grad: 3.5531  LR: 0.00001998  \n",
      "Epoch: [1][700/5462] Elapsed 4m 57s (remain 33m 38s) Loss: 0.6007(0.6672) Grad: 2.0655  LR: 0.00001997  \n",
      "Epoch: [1][800/5462] Elapsed 5m 39s (remain 32m 56s) Loss: 0.6407(0.6631) Grad: 2.3286  LR: 0.00001996  \n",
      "Epoch: [1][900/5462] Elapsed 6m 21s (remain 32m 13s) Loss: 0.6041(0.6596) Grad: 2.5947  LR: 0.00001995  \n",
      "Epoch: [1][1000/5462] Elapsed 7m 4s (remain 31m 31s) Loss: 0.6527(0.6559) Grad: 2.9965  LR: 0.00001993  \n",
      "Epoch: [1][1100/5462] Elapsed 7m 46s (remain 30m 49s) Loss: 0.6962(0.6537) Grad: 5.3244  LR: 0.00001992  \n",
      "Epoch: [1][1200/5462] Elapsed 8m 29s (remain 30m 7s) Loss: 0.5405(0.6518) Grad: 2.1158  LR: 0.00001990  \n",
      "Epoch: [1][1300/5462] Elapsed 9m 11s (remain 29m 24s) Loss: 0.6641(0.6495) Grad: 5.4593  LR: 0.00001989  \n",
      "Epoch: [1][1400/5462] Elapsed 9m 54s (remain 28m 42s) Loss: 0.6416(0.6470) Grad: 3.0487  LR: 0.00001987  \n",
      "Epoch: [1][1500/5462] Elapsed 10m 36s (remain 28m 0s) Loss: 0.5685(0.6449) Grad: 2.1530  LR: 0.00001985  \n",
      "Epoch: [1][1600/5462] Elapsed 11m 19s (remain 27m 17s) Loss: 0.6118(0.6426) Grad: 3.7395  LR: 0.00001983  \n",
      "Epoch: [1][1700/5462] Elapsed 12m 1s (remain 26m 35s) Loss: 0.6422(0.6410) Grad: 2.8263  LR: 0.00001981  \n",
      "Epoch: [1][1800/5462] Elapsed 12m 43s (remain 25m 52s) Loss: 0.5469(0.6390) Grad: 2.4756  LR: 0.00001979  \n",
      "Epoch: [1][1900/5462] Elapsed 13m 26s (remain 25m 10s) Loss: 0.7565(0.6374) Grad: 4.4762  LR: 0.00001976  \n",
      "Epoch: [1][2000/5462] Elapsed 14m 8s (remain 24m 28s) Loss: 0.6005(0.6359) Grad: 3.4509  LR: 0.00001974  \n",
      "Epoch: [1][2100/5462] Elapsed 14m 51s (remain 23m 45s) Loss: 0.6517(0.6344) Grad: 3.9305  LR: 0.00001971  \n",
      "Epoch: [1][2200/5462] Elapsed 15m 33s (remain 23m 3s) Loss: 0.4777(0.6329) Grad: 2.7464  LR: 0.00001968  \n",
      "Epoch: [1][2300/5462] Elapsed 16m 16s (remain 22m 20s) Loss: 0.5487(0.6314) Grad: 3.1519  LR: 0.00001965  \n",
      "Epoch: [1][2400/5462] Elapsed 16m 58s (remain 21m 38s) Loss: 0.6565(0.6299) Grad: 3.2985  LR: 0.00001962  \n",
      "Epoch: [1][2500/5462] Elapsed 17m 40s (remain 20m 56s) Loss: 0.7783(0.6286) Grad: 5.9729  LR: 0.00001959  \n",
      "Epoch: [1][2600/5462] Elapsed 18m 23s (remain 20m 13s) Loss: 0.6160(0.6271) Grad: 3.2698  LR: 0.00001956  \n",
      "Epoch: [1][2700/5462] Elapsed 19m 5s (remain 19m 31s) Loss: 0.5337(0.6257) Grad: 2.2843  LR: 0.00001952  \n",
      "Epoch: [1][2800/5462] Elapsed 19m 48s (remain 18m 48s) Loss: 0.5213(0.6247) Grad: 4.1472  LR: 0.00001949  \n",
      "Epoch: [1][2900/5462] Elapsed 20m 30s (remain 18m 6s) Loss: 0.6676(0.6236) Grad: 4.3562  LR: 0.00001945  \n",
      "Epoch: [1][3000/5462] Elapsed 21m 13s (remain 17m 24s) Loss: 0.5950(0.6226) Grad: 2.7994  LR: 0.00001941  \n",
      "Epoch: [1][3100/5462] Elapsed 21m 55s (remain 16m 41s) Loss: 0.6018(0.6214) Grad: 4.5335  LR: 0.00001937  \n",
      "Epoch: [1][3200/5462] Elapsed 22m 38s (remain 15m 59s) Loss: 0.5951(0.6203) Grad: 2.8747  LR: 0.00001933  \n",
      "Epoch: [1][3300/5462] Elapsed 23m 20s (remain 15m 16s) Loss: 0.5422(0.6192) Grad: 2.3149  LR: 0.00001929  \n",
      "Epoch: [1][3400/5462] Elapsed 24m 3s (remain 14m 34s) Loss: 0.5774(0.6181) Grad: 2.8808  LR: 0.00001924  \n",
      "Epoch: [1][3500/5462] Elapsed 24m 45s (remain 13m 52s) Loss: 0.6632(0.6169) Grad: 4.1874  LR: 0.00001920  \n",
      "Epoch: [1][3600/5462] Elapsed 25m 28s (remain 13m 9s) Loss: 0.5166(0.6160) Grad: 3.3905  LR: 0.00001915  \n",
      "Epoch: [1][3700/5462] Elapsed 26m 10s (remain 12m 27s) Loss: 0.5868(0.6152) Grad: 4.5852  LR: 0.00001911  \n",
      "Epoch: [1][3800/5462] Elapsed 26m 53s (remain 11m 44s) Loss: 0.5564(0.6145) Grad: 5.1243  LR: 0.00001906  \n",
      "Epoch: [1][3900/5462] Elapsed 27m 35s (remain 11m 2s) Loss: 0.5273(0.6135) Grad: 1.8162  LR: 0.00001901  \n",
      "Epoch: [1][4000/5462] Elapsed 28m 18s (remain 10m 20s) Loss: 0.6315(0.6127) Grad: 2.5982  LR: 0.00001896  \n",
      "Epoch: [1][4100/5462] Elapsed 29m 0s (remain 9m 37s) Loss: 0.5407(0.6119) Grad: 4.8657  LR: 0.00001891  \n",
      "Epoch: [1][4200/5462] Elapsed 29m 43s (remain 8m 55s) Loss: 0.5070(0.6111) Grad: 1.8012  LR: 0.00001886  \n",
      "Epoch: [1][4300/5462] Elapsed 30m 25s (remain 8m 12s) Loss: 0.5992(0.6104) Grad: 3.9711  LR: 0.00001880  \n",
      "Epoch: [1][4400/5462] Elapsed 31m 8s (remain 7m 30s) Loss: 0.5927(0.6098) Grad: 3.4201  LR: 0.00001875  \n",
      "Epoch: [1][4500/5462] Elapsed 31m 50s (remain 6m 47s) Loss: 0.6394(0.6088) Grad: 2.3275  LR: 0.00001869  \n",
      "Epoch: [1][4600/5462] Elapsed 32m 33s (remain 6m 5s) Loss: 0.6369(0.6082) Grad: 9.5415  LR: 0.00001863  \n",
      "Epoch: [1][4700/5462] Elapsed 33m 15s (remain 5m 23s) Loss: 0.6753(0.6077) Grad: 4.9435  LR: 0.00001857  \n",
      "Epoch: [1][4800/5462] Elapsed 33m 58s (remain 4m 40s) Loss: 0.5802(0.6070) Grad: 3.0810  LR: 0.00001851  \n",
      "Epoch: [1][4900/5462] Elapsed 34m 40s (remain 3m 58s) Loss: 0.4521(0.6061) Grad: 3.3517  LR: 0.00001845  \n",
      "Epoch: [1][5000/5462] Elapsed 35m 23s (remain 3m 15s) Loss: 0.5575(0.6054) Grad: 4.6307  LR: 0.00001839  \n",
      "Epoch: [1][5100/5462] Elapsed 36m 5s (remain 2m 33s) Loss: 0.5527(0.6050) Grad: 3.1146  LR: 0.00001833  \n",
      "Epoch: [1][5200/5462] Elapsed 36m 48s (remain 1m 50s) Loss: 0.5959(0.6044) Grad: 2.2374  LR: 0.00001826  \n",
      "Epoch: [1][5300/5462] Elapsed 37m 30s (remain 1m 8s) Loss: 0.5313(0.6040) Grad: 2.1028  LR: 0.00001820  \n",
      "Epoch: [1][5400/5462] Elapsed 38m 13s (remain 0m 25s) Loss: 0.6049(0.6034) Grad: 3.3942  LR: 0.00001813  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.6030  time: 2319s\n",
      "Epoch 1 - Save Best Score: 0.6030 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][5461/5462] Elapsed 38m 39s (remain 0m 0s) Loss: 0.5641(0.6030) Grad: 3.6003  LR: 0.00001809  \n",
      "Epoch: [2][0/5462] Elapsed 0m 0s (remain 55m 21s) Loss: 0.4985(0.4985) Grad: 2.0655  LR: 0.00001809  \n",
      "Epoch: [2][100/5462] Elapsed 0m 43s (remain 38m 7s) Loss: 0.6030(0.5532) Grad: 2.9004  LR: 0.00001802  \n",
      "Epoch: [2][200/5462] Elapsed 1m 25s (remain 37m 19s) Loss: 0.5940(0.5610) Grad: 2.6174  LR: 0.00001795  \n",
      "Epoch: [2][300/5462] Elapsed 2m 8s (remain 36m 37s) Loss: 0.5607(0.5586) Grad: 4.3679  LR: 0.00001788  \n",
      "Epoch: [2][400/5462] Elapsed 2m 50s (remain 35m 53s) Loss: 0.6172(0.5604) Grad: 3.9766  LR: 0.00001781  \n",
      "Epoch: [2][500/5462] Elapsed 3m 33s (remain 35m 10s) Loss: 0.4567(0.5599) Grad: 2.4254  LR: 0.00001774  \n",
      "Epoch: [2][600/5462] Elapsed 4m 15s (remain 34m 28s) Loss: 0.5883(0.5597) Grad: 2.5582  LR: 0.00001767  \n",
      "Epoch: [2][700/5462] Elapsed 4m 58s (remain 33m 45s) Loss: 0.5977(0.5610) Grad: 3.1096  LR: 0.00001759  \n",
      "Epoch: [2][800/5462] Elapsed 5m 40s (remain 33m 2s) Loss: 0.5673(0.5607) Grad: 3.2534  LR: 0.00001752  \n",
      "Epoch: [2][900/5462] Elapsed 6m 23s (remain 32m 19s) Loss: 0.5798(0.5609) Grad: 3.2891  LR: 0.00001744  \n",
      "Epoch: [2][1000/5462] Elapsed 7m 5s (remain 31m 37s) Loss: 0.6217(0.5611) Grad: 5.0754  LR: 0.00001736  \n",
      "Epoch: [2][1100/5462] Elapsed 7m 48s (remain 30m 54s) Loss: 0.5234(0.5610) Grad: 1.9000  LR: 0.00001728  \n",
      "Epoch: [2][1200/5462] Elapsed 8m 30s (remain 30m 11s) Loss: 0.4715(0.5599) Grad: 2.5047  LR: 0.00001720  \n",
      "Epoch: [2][1300/5462] Elapsed 9m 13s (remain 29m 29s) Loss: 0.5711(0.5602) Grad: 4.3322  LR: 0.00001712  \n",
      "Epoch: [2][1400/5462] Elapsed 9m 55s (remain 28m 46s) Loss: 0.5978(0.5600) Grad: 4.8775  LR: 0.00001704  \n",
      "Epoch: [2][1500/5462] Elapsed 10m 38s (remain 28m 4s) Loss: 0.5317(0.5606) Grad: 2.7636  LR: 0.00001696  \n",
      "Epoch: [2][1600/5462] Elapsed 11m 20s (remain 27m 21s) Loss: 0.5770(0.5595) Grad: 3.3390  LR: 0.00001688  \n",
      "Epoch: [2][1700/5462] Elapsed 12m 3s (remain 26m 39s) Loss: 0.4728(0.5592) Grad: 2.9266  LR: 0.00001679  \n",
      "Epoch: [2][1800/5462] Elapsed 12m 45s (remain 25m 56s) Loss: 0.5960(0.5590) Grad: 2.3645  LR: 0.00001671  \n",
      "Epoch: [2][1900/5462] Elapsed 13m 28s (remain 25m 14s) Loss: 0.5625(0.5589) Grad: 2.0481  LR: 0.00001662  \n",
      "Epoch: [2][2000/5462] Elapsed 14m 10s (remain 24m 31s) Loss: 0.5481(0.5584) Grad: 3.0082  LR: 0.00001654  \n",
      "Epoch: [2][2100/5462] Elapsed 14m 53s (remain 23m 48s) Loss: 0.4061(0.5582) Grad: 2.6385  LR: 0.00001645  \n",
      "Epoch: [2][2200/5462] Elapsed 15m 35s (remain 23m 6s) Loss: 0.5974(0.5581) Grad: 5.4047  LR: 0.00001636  \n",
      "Epoch: [2][2300/5462] Elapsed 16m 18s (remain 22m 23s) Loss: 0.5724(0.5576) Grad: 1.9929  LR: 0.00001627  \n",
      "Epoch: [2][2400/5462] Elapsed 17m 0s (remain 21m 41s) Loss: 0.5394(0.5569) Grad: 5.7304  LR: 0.00001618  \n",
      "Epoch: [2][2500/5462] Elapsed 17m 43s (remain 20m 58s) Loss: 0.5792(0.5564) Grad: 3.1784  LR: 0.00001609  \n",
      "Epoch: [2][2600/5462] Elapsed 18m 25s (remain 20m 16s) Loss: 0.5761(0.5563) Grad: 2.6042  LR: 0.00001600  \n",
      "Epoch: [2][2700/5462] Elapsed 19m 8s (remain 19m 33s) Loss: 0.6186(0.5560) Grad: 7.8453  LR: 0.00001591  \n",
      "Epoch: [2][2800/5462] Elapsed 19m 50s (remain 18m 51s) Loss: 0.5228(0.5561) Grad: 2.7035  LR: 0.00001581  \n",
      "Epoch: [2][2900/5462] Elapsed 20m 33s (remain 18m 8s) Loss: 0.5125(0.5563) Grad: 2.6158  LR: 0.00001572  \n",
      "Epoch: [2][3000/5462] Elapsed 21m 15s (remain 17m 26s) Loss: 0.4482(0.5564) Grad: 1.4766  LR: 0.00001562  \n",
      "Epoch: [2][3100/5462] Elapsed 21m 58s (remain 16m 43s) Loss: 0.5631(0.5562) Grad: 1.6604  LR: 0.00001553  \n",
      "Epoch: [2][3200/5462] Elapsed 22m 40s (remain 16m 1s) Loss: 0.6274(0.5562) Grad: 3.8914  LR: 0.00001543  \n",
      "Epoch: [2][3300/5462] Elapsed 23m 23s (remain 15m 18s) Loss: 0.5195(0.5563) Grad: 3.7521  LR: 0.00001534  \n",
      "Epoch: [2][3400/5462] Elapsed 24m 5s (remain 14m 36s) Loss: 0.4985(0.5563) Grad: 1.6938  LR: 0.00001524  \n",
      "Epoch: [2][3500/5462] Elapsed 24m 48s (remain 13m 53s) Loss: 0.5139(0.5558) Grad: 3.5954  LR: 0.00001514  \n",
      "Epoch: [2][3600/5462] Elapsed 25m 30s (remain 13m 11s) Loss: 0.5442(0.5557) Grad: 2.6239  LR: 0.00001504  \n",
      "Epoch: [2][3700/5462] Elapsed 26m 13s (remain 12m 28s) Loss: 0.6284(0.5557) Grad: 2.2858  LR: 0.00001494  \n",
      "Epoch: [2][3800/5462] Elapsed 26m 56s (remain 11m 46s) Loss: 0.5841(0.5556) Grad: 2.8083  LR: 0.00001484  \n",
      "Epoch: [2][3900/5462] Elapsed 27m 38s (remain 11m 3s) Loss: 0.5312(0.5556) Grad: 1.6745  LR: 0.00001474  \n",
      "Epoch: [2][4000/5462] Elapsed 28m 21s (remain 10m 21s) Loss: 0.5856(0.5555) Grad: 3.9339  LR: 0.00001464  \n",
      "Epoch: [2][4100/5462] Elapsed 29m 3s (remain 9m 38s) Loss: 0.5684(0.5554) Grad: 2.0882  LR: 0.00001454  \n",
      "Epoch: [2][4200/5462] Elapsed 29m 46s (remain 8m 56s) Loss: 0.5961(0.5552) Grad: 2.5061  LR: 0.00001443  \n",
      "Epoch: [2][4300/5462] Elapsed 30m 28s (remain 8m 13s) Loss: 0.6769(0.5552) Grad: 3.4034  LR: 0.00001433  \n",
      "Epoch: [2][4400/5462] Elapsed 31m 11s (remain 7m 31s) Loss: 0.5407(0.5553) Grad: 3.6184  LR: 0.00001423  \n",
      "Epoch: [2][4500/5462] Elapsed 31m 53s (remain 6m 48s) Loss: 0.5342(0.5553) Grad: 5.3815  LR: 0.00001412  \n",
      "Epoch: [2][4600/5462] Elapsed 32m 36s (remain 6m 6s) Loss: 0.5683(0.5551) Grad: 3.0115  LR: 0.00001402  \n",
      "Epoch: [2][4700/5462] Elapsed 33m 18s (remain 5m 23s) Loss: 0.4861(0.5550) Grad: 3.3980  LR: 0.00001391  \n",
      "Epoch: [2][4800/5462] Elapsed 34m 1s (remain 4m 41s) Loss: 0.4878(0.5549) Grad: 3.8309  LR: 0.00001381  \n",
      "Epoch: [2][4900/5462] Elapsed 34m 43s (remain 3m 58s) Loss: 0.5161(0.5548) Grad: 5.5079  LR: 0.00001370  \n",
      "Epoch: [2][5000/5462] Elapsed 35m 26s (remain 3m 16s) Loss: 0.6396(0.5549) Grad: 4.1804  LR: 0.00001359  \n",
      "Epoch: [2][5100/5462] Elapsed 36m 9s (remain 2m 33s) Loss: 0.5413(0.5547) Grad: 3.6110  LR: 0.00001348  \n",
      "Epoch: [2][5200/5462] Elapsed 36m 51s (remain 1m 50s) Loss: 0.4896(0.5545) Grad: 2.2040  LR: 0.00001338  \n",
      "Epoch: [2][5300/5462] Elapsed 37m 34s (remain 1m 8s) Loss: 0.5413(0.5544) Grad: 2.3542  LR: 0.00001327  \n",
      "Epoch: [2][5400/5462] Elapsed 38m 16s (remain 0m 25s) Loss: 0.5476(0.5542) Grad: 2.7260  LR: 0.00001316  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.5541  time: 2323s\n",
      "Epoch 2 - Save Best Score: 0.5541 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][5461/5462] Elapsed 38m 42s (remain 0m 0s) Loss: 0.4471(0.5541) Grad: 3.4686  LR: 0.00001309  \n",
      "Epoch: [3][0/5462] Elapsed 0m 0s (remain 60m 8s) Loss: 0.5879(0.5879) Grad: 1.6557  LR: 0.00001309  \n",
      "Epoch: [3][100/5462] Elapsed 0m 43s (remain 38m 11s) Loss: 0.4373(0.5270) Grad: 2.9245  LR: 0.00001298  \n",
      "Epoch: [3][200/5462] Elapsed 1m 25s (remain 37m 22s) Loss: 0.4134(0.5348) Grad: 4.8092  LR: 0.00001287  \n",
      "Epoch: [3][300/5462] Elapsed 2m 8s (remain 36m 38s) Loss: 0.5188(0.5328) Grad: 1.9432  LR: 0.00001276  \n",
      "Epoch: [3][400/5462] Elapsed 2m 50s (remain 35m 54s) Loss: 0.5821(0.5321) Grad: 5.2909  LR: 0.00001265  \n",
      "Epoch: [3][500/5462] Elapsed 3m 33s (remain 35m 11s) Loss: 0.5218(0.5332) Grad: 2.2399  LR: 0.00001254  \n",
      "Epoch: [3][600/5462] Elapsed 4m 15s (remain 34m 29s) Loss: 0.4573(0.5335) Grad: 2.3221  LR: 0.00001243  \n",
      "Epoch: [3][700/5462] Elapsed 4m 58s (remain 33m 47s) Loss: 0.4638(0.5334) Grad: 3.2822  LR: 0.00001232  \n",
      "Epoch: [3][800/5462] Elapsed 5m 41s (remain 33m 4s) Loss: 0.5329(0.5339) Grad: 2.8034  LR: 0.00001220  \n",
      "Epoch: [3][900/5462] Elapsed 6m 23s (remain 32m 21s) Loss: 0.5390(0.5325) Grad: 2.5523  LR: 0.00001209  \n",
      "Epoch: [3][1000/5462] Elapsed 7m 6s (remain 31m 38s) Loss: 0.5145(0.5332) Grad: 2.1760  LR: 0.00001198  \n",
      "Epoch: [3][1100/5462] Elapsed 7m 48s (remain 30m 56s) Loss: 0.5937(0.5336) Grad: 5.4461  LR: 0.00001187  \n",
      "Epoch: [3][1200/5462] Elapsed 8m 31s (remain 30m 13s) Loss: 0.5206(0.5339) Grad: 3.2487  LR: 0.00001175  \n",
      "Epoch: [3][1300/5462] Elapsed 9m 13s (remain 29m 30s) Loss: 0.5151(0.5333) Grad: 3.5266  LR: 0.00001164  \n",
      "Epoch: [3][1400/5462] Elapsed 9m 56s (remain 28m 48s) Loss: 0.6151(0.5335) Grad: 2.6853  LR: 0.00001153  \n",
      "Epoch: [3][1500/5462] Elapsed 10m 38s (remain 28m 5s) Loss: 0.6025(0.5343) Grad: 2.5029  LR: 0.00001141  \n",
      "Epoch: [3][1600/5462] Elapsed 11m 21s (remain 27m 23s) Loss: 0.5250(0.5343) Grad: 3.3796  LR: 0.00001130  \n",
      "Epoch: [3][1700/5462] Elapsed 12m 3s (remain 26m 40s) Loss: 0.4411(0.5347) Grad: 2.6672  LR: 0.00001118  \n",
      "Epoch: [3][1800/5462] Elapsed 12m 46s (remain 25m 57s) Loss: 0.4765(0.5341) Grad: 2.7825  LR: 0.00001107  \n",
      "Epoch: [3][1900/5462] Elapsed 13m 29s (remain 25m 15s) Loss: 0.4971(0.5349) Grad: 3.2615  LR: 0.00001096  \n",
      "Epoch: [3][2000/5462] Elapsed 14m 11s (remain 24m 32s) Loss: 0.5082(0.5349) Grad: 2.2244  LR: 0.00001084  \n",
      "Epoch: [3][2100/5462] Elapsed 14m 54s (remain 23m 50s) Loss: 0.5385(0.5345) Grad: 3.4847  LR: 0.00001073  \n",
      "Epoch: [3][2200/5462] Elapsed 15m 36s (remain 23m 7s) Loss: 0.4311(0.5343) Grad: 2.3874  LR: 0.00001061  \n",
      "Epoch: [3][2300/5462] Elapsed 16m 19s (remain 22m 25s) Loss: 0.5092(0.5342) Grad: 3.4239  LR: 0.00001050  \n",
      "Epoch: [3][2400/5462] Elapsed 17m 1s (remain 21m 42s) Loss: 0.4389(0.5340) Grad: 2.8628  LR: 0.00001038  \n",
      "Epoch: [3][2500/5462] Elapsed 17m 44s (remain 21m 0s) Loss: 0.6447(0.5337) Grad: 3.4761  LR: 0.00001027  \n",
      "Epoch: [3][2600/5462] Elapsed 18m 26s (remain 20m 17s) Loss: 0.5627(0.5331) Grad: 2.9953  LR: 0.00001015  \n",
      "Epoch: [3][2700/5462] Elapsed 19m 9s (remain 19m 34s) Loss: 0.5624(0.5330) Grad: 2.3840  LR: 0.00001004  \n",
      "Epoch: [3][2800/5462] Elapsed 19m 51s (remain 18m 52s) Loss: 0.5709(0.5329) Grad: 3.2018  LR: 0.00000992  \n",
      "Epoch: [3][2900/5462] Elapsed 20m 34s (remain 18m 9s) Loss: 0.5535(0.5326) Grad: 2.5837  LR: 0.00000981  \n",
      "Epoch: [3][3000/5462] Elapsed 21m 17s (remain 17m 27s) Loss: 0.5288(0.5325) Grad: 4.4215  LR: 0.00000969  \n",
      "Epoch: [3][3100/5462] Elapsed 21m 59s (remain 16m 44s) Loss: 0.6071(0.5328) Grad: 3.6372  LR: 0.00000958  \n",
      "Epoch: [3][3200/5462] Elapsed 22m 42s (remain 16m 2s) Loss: 0.4594(0.5326) Grad: 2.2269  LR: 0.00000946  \n",
      "Epoch: [3][3300/5462] Elapsed 23m 24s (remain 15m 19s) Loss: 0.5765(0.5328) Grad: 1.7056  LR: 0.00000935  \n",
      "Epoch: [3][3400/5462] Elapsed 24m 7s (remain 14m 37s) Loss: 0.4509(0.5325) Grad: 4.2246  LR: 0.00000923  \n",
      "Epoch: [3][3500/5462] Elapsed 24m 49s (remain 13m 54s) Loss: 0.6211(0.5327) Grad: 2.4192  LR: 0.00000912  \n",
      "Epoch: [3][3600/5462] Elapsed 25m 32s (remain 13m 11s) Loss: 0.5941(0.5326) Grad: 2.0890  LR: 0.00000900  \n",
      "Epoch: [3][3700/5462] Elapsed 26m 14s (remain 12m 29s) Loss: 0.5665(0.5323) Grad: 2.9890  LR: 0.00000889  \n",
      "Epoch: [3][3800/5462] Elapsed 26m 57s (remain 11m 46s) Loss: 0.5554(0.5322) Grad: 5.0395  LR: 0.00000877  \n",
      "Epoch: [3][3900/5462] Elapsed 27m 39s (remain 11m 4s) Loss: 0.5164(0.5325) Grad: 4.4213  LR: 0.00000866  \n",
      "Epoch: [3][4000/5462] Elapsed 28m 22s (remain 10m 21s) Loss: 0.6486(0.5326) Grad: 4.8871  LR: 0.00000855  \n",
      "Epoch: [3][4100/5462] Elapsed 29m 5s (remain 9m 39s) Loss: 0.6795(0.5324) Grad: 2.7391  LR: 0.00000843  \n",
      "Epoch: [3][4200/5462] Elapsed 29m 47s (remain 8m 56s) Loss: 0.6302(0.5323) Grad: 2.3766  LR: 0.00000832  \n",
      "Epoch: [3][4300/5462] Elapsed 30m 30s (remain 8m 14s) Loss: 0.4665(0.5323) Grad: 3.0670  LR: 0.00000821  \n",
      "Epoch: [3][4400/5462] Elapsed 31m 12s (remain 7m 31s) Loss: 0.5489(0.5321) Grad: 2.1883  LR: 0.00000809  \n",
      "Epoch: [3][4500/5462] Elapsed 31m 55s (remain 6m 48s) Loss: 0.5445(0.5322) Grad: 2.8202  LR: 0.00000798  \n",
      "Epoch: [3][4600/5462] Elapsed 32m 37s (remain 6m 6s) Loss: 0.4079(0.5324) Grad: 2.3043  LR: 0.00000787  \n",
      "Epoch: [3][4700/5462] Elapsed 33m 20s (remain 5m 23s) Loss: 0.5286(0.5323) Grad: 2.3294  LR: 0.00000776  \n",
      "Epoch: [3][4800/5462] Elapsed 34m 3s (remain 4m 41s) Loss: 0.4385(0.5323) Grad: 2.8168  LR: 0.00000764  \n",
      "Epoch: [3][4900/5462] Elapsed 34m 45s (remain 3m 58s) Loss: 0.6794(0.5324) Grad: 3.1560  LR: 0.00000753  \n",
      "Epoch: [3][5000/5462] Elapsed 35m 28s (remain 3m 16s) Loss: 0.5455(0.5322) Grad: 2.1983  LR: 0.00000742  \n",
      "Epoch: [3][5100/5462] Elapsed 36m 10s (remain 2m 33s) Loss: 0.4716(0.5323) Grad: 4.3867  LR: 0.00000731  \n",
      "Epoch: [3][5200/5462] Elapsed 36m 53s (remain 1m 51s) Loss: 0.5133(0.5324) Grad: 1.5844  LR: 0.00000720  \n",
      "Epoch: [3][5300/5462] Elapsed 37m 35s (remain 1m 8s) Loss: 0.5450(0.5321) Grad: 3.0911  LR: 0.00000709  \n",
      "Epoch: [3][5400/5462] Elapsed 38m 18s (remain 0m 25s) Loss: 0.5796(0.5321) Grad: 3.1431  LR: 0.00000698  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.5320  time: 2325s\n",
      "Epoch 3 - Save Best Score: 0.5320 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][5461/5462] Elapsed 38m 44s (remain 0m 0s) Loss: 0.5388(0.5320) Grad: 3.8074  LR: 0.00000691  \n",
      "Epoch: [4][0/5462] Elapsed 0m 0s (remain 77m 5s) Loss: 0.4664(0.4664) Grad: 2.6003  LR: 0.00000691  \n",
      "Epoch: [4][100/5462] Elapsed 0m 43s (remain 38m 22s) Loss: 0.4231(0.5169) Grad: 1.8408  LR: 0.00000680  \n",
      "Epoch: [4][200/5462] Elapsed 1m 25s (remain 37m 28s) Loss: 0.5118(0.5163) Grad: 3.6148  LR: 0.00000669  \n",
      "Epoch: [4][300/5462] Elapsed 2m 8s (remain 36m 42s) Loss: 0.5304(0.5172) Grad: 5.5566  LR: 0.00000659  \n",
      "Epoch: [4][400/5462] Elapsed 2m 51s (remain 35m 58s) Loss: 0.5024(0.5169) Grad: 2.0228  LR: 0.00000648  \n",
      "Epoch: [4][500/5462] Elapsed 3m 33s (remain 35m 15s) Loss: 0.4276(0.5187) Grad: 2.5384  LR: 0.00000637  \n",
      "Epoch: [4][600/5462] Elapsed 4m 16s (remain 34m 31s) Loss: 0.4724(0.5184) Grad: 2.1307  LR: 0.00000626  \n",
      "Epoch: [4][700/5462] Elapsed 4m 58s (remain 33m 48s) Loss: 0.6063(0.5198) Grad: 2.9034  LR: 0.00000616  \n",
      "Epoch: [4][800/5462] Elapsed 5m 41s (remain 33m 5s) Loss: 0.5758(0.5196) Grad: 3.2953  LR: 0.00000605  \n",
      "Epoch: [4][900/5462] Elapsed 6m 23s (remain 32m 23s) Loss: 0.3337(0.5191) Grad: 5.3175  LR: 0.00000595  \n",
      "Epoch: [4][1000/5462] Elapsed 7m 6s (remain 31m 40s) Loss: 0.5419(0.5186) Grad: 3.1334  LR: 0.00000584  \n",
      "Epoch: [4][1100/5462] Elapsed 7m 49s (remain 30m 58s) Loss: 0.4717(0.5186) Grad: 2.5748  LR: 0.00000574  \n",
      "Epoch: [4][1200/5462] Elapsed 8m 31s (remain 30m 15s) Loss: 0.6480(0.5192) Grad: 3.6903  LR: 0.00000563  \n",
      "Epoch: [4][1300/5462] Elapsed 9m 14s (remain 29m 32s) Loss: 0.5647(0.5183) Grad: 2.8987  LR: 0.00000553  \n",
      "Epoch: [4][1400/5462] Elapsed 9m 56s (remain 28m 50s) Loss: 0.5383(0.5187) Grad: 4.1338  LR: 0.00000543  \n",
      "Epoch: [4][1500/5462] Elapsed 10m 39s (remain 28m 7s) Loss: 0.4637(0.5188) Grad: 2.9630  LR: 0.00000532  \n",
      "Epoch: [4][1600/5462] Elapsed 11m 22s (remain 27m 24s) Loss: 0.5706(0.5192) Grad: 3.5370  LR: 0.00000522  \n",
      "Epoch: [4][1700/5462] Elapsed 12m 4s (remain 26m 42s) Loss: 0.6428(0.5188) Grad: 3.3239  LR: 0.00000512  \n",
      "Epoch: [4][1800/5462] Elapsed 12m 47s (remain 25m 59s) Loss: 0.4997(0.5188) Grad: 2.7948  LR: 0.00000502  \n",
      "Epoch: [4][1900/5462] Elapsed 13m 29s (remain 25m 16s) Loss: 0.5668(0.5187) Grad: 3.3070  LR: 0.00000492  \n",
      "Epoch: [4][2000/5462] Elapsed 14m 12s (remain 24m 34s) Loss: 0.5362(0.5188) Grad: 2.2661  LR: 0.00000482  \n",
      "Epoch: [4][2100/5462] Elapsed 14m 54s (remain 23m 51s) Loss: 0.4173(0.5186) Grad: 3.5012  LR: 0.00000473  \n",
      "Epoch: [4][2200/5462] Elapsed 15m 37s (remain 23m 9s) Loss: 0.4783(0.5186) Grad: 2.6722  LR: 0.00000463  \n",
      "Epoch: [4][2300/5462] Elapsed 16m 20s (remain 22m 26s) Loss: 0.4704(0.5183) Grad: 5.2310  LR: 0.00000453  \n",
      "Epoch: [4][2400/5462] Elapsed 17m 2s (remain 21m 43s) Loss: 0.4883(0.5180) Grad: 2.2898  LR: 0.00000444  \n",
      "Epoch: [4][2500/5462] Elapsed 17m 45s (remain 21m 1s) Loss: 0.4258(0.5179) Grad: 2.6649  LR: 0.00000434  \n",
      "Epoch: [4][2600/5462] Elapsed 18m 27s (remain 20m 18s) Loss: 0.4737(0.5180) Grad: 2.3545  LR: 0.00000425  \n",
      "Epoch: [4][2700/5462] Elapsed 19m 10s (remain 19m 36s) Loss: 0.3484(0.5177) Grad: 4.4890  LR: 0.00000415  \n",
      "Epoch: [4][2800/5462] Elapsed 19m 53s (remain 18m 53s) Loss: 0.5003(0.5179) Grad: 4.3092  LR: 0.00000406  \n",
      "Epoch: [4][2900/5462] Elapsed 20m 35s (remain 18m 10s) Loss: 0.3897(0.5177) Grad: 2.1976  LR: 0.00000397  \n",
      "Epoch: [4][3000/5462] Elapsed 21m 18s (remain 17m 28s) Loss: 0.6409(0.5177) Grad: 5.9106  LR: 0.00000388  \n",
      "Epoch: [4][3100/5462] Elapsed 22m 0s (remain 16m 45s) Loss: 0.4284(0.5174) Grad: 2.8601  LR: 0.00000379  \n",
      "Epoch: [4][3200/5462] Elapsed 22m 43s (remain 16m 3s) Loss: 0.5792(0.5175) Grad: 4.6349  LR: 0.00000370  \n",
      "Epoch: [4][3300/5462] Elapsed 23m 25s (remain 15m 20s) Loss: 0.4970(0.5172) Grad: 4.1707  LR: 0.00000361  \n",
      "Epoch: [4][3400/5462] Elapsed 24m 8s (remain 14m 37s) Loss: 0.5809(0.5170) Grad: 2.9389  LR: 0.00000352  \n",
      "Epoch: [4][3500/5462] Elapsed 24m 51s (remain 13m 55s) Loss: 0.6249(0.5172) Grad: 2.2621  LR: 0.00000343  \n",
      "Epoch: [4][3600/5462] Elapsed 25m 33s (remain 13m 12s) Loss: 0.6550(0.5172) Grad: 5.1080  LR: 0.00000335  \n",
      "Epoch: [4][3700/5462] Elapsed 26m 16s (remain 12m 30s) Loss: 0.5356(0.5171) Grad: 3.2974  LR: 0.00000326  \n",
      "Epoch: [4][3800/5462] Elapsed 26m 59s (remain 11m 47s) Loss: 0.6186(0.5173) Grad: 2.5426  LR: 0.00000318  \n",
      "Epoch: [4][3900/5462] Elapsed 27m 41s (remain 11m 4s) Loss: 0.4198(0.5168) Grad: 2.3403  LR: 0.00000309  \n",
      "Epoch: [4][4000/5462] Elapsed 28m 24s (remain 10m 22s) Loss: 0.5326(0.5168) Grad: 1.7462  LR: 0.00000301  \n",
      "Epoch: [4][4100/5462] Elapsed 29m 6s (remain 9m 39s) Loss: 0.5151(0.5166) Grad: 4.7015  LR: 0.00000293  \n",
      "Epoch: [4][4200/5462] Elapsed 29m 49s (remain 8m 57s) Loss: 0.4774(0.5167) Grad: 3.2683  LR: 0.00000285  \n",
      "Epoch: [4][4300/5462] Elapsed 30m 31s (remain 8m 14s) Loss: 0.5388(0.5167) Grad: 2.5658  LR: 0.00000277  \n",
      "Epoch: [4][4400/5462] Elapsed 31m 14s (remain 7m 31s) Loss: 0.4233(0.5166) Grad: 2.5029  LR: 0.00000269  \n",
      "Epoch: [4][4500/5462] Elapsed 31m 57s (remain 6m 49s) Loss: 0.4395(0.5164) Grad: 1.7454  LR: 0.00000261  \n",
      "Epoch: [4][4600/5462] Elapsed 32m 39s (remain 6m 6s) Loss: 0.5065(0.5164) Grad: 3.6639  LR: 0.00000253  \n",
      "Epoch: [4][4700/5462] Elapsed 33m 22s (remain 5m 24s) Loss: 0.4634(0.5163) Grad: 2.2466  LR: 0.00000246  \n",
      "Epoch: [4][4800/5462] Elapsed 34m 4s (remain 4m 41s) Loss: 0.4903(0.5162) Grad: 2.4322  LR: 0.00000238  \n",
      "Epoch: [4][4900/5462] Elapsed 34m 47s (remain 3m 58s) Loss: 0.6384(0.5162) Grad: 4.0807  LR: 0.00000231  \n",
      "Epoch: [4][5000/5462] Elapsed 35m 29s (remain 3m 16s) Loss: 0.4456(0.5162) Grad: 2.0343  LR: 0.00000224  \n",
      "Epoch: [4][5100/5462] Elapsed 36m 12s (remain 2m 33s) Loss: 0.3804(0.5161) Grad: 3.0558  LR: 0.00000216  \n",
      "Epoch: [4][5200/5462] Elapsed 36m 55s (remain 1m 51s) Loss: 0.4787(0.5161) Grad: 1.5968  LR: 0.00000209  \n",
      "Epoch: [4][5300/5462] Elapsed 37m 37s (remain 1m 8s) Loss: 0.4542(0.5160) Grad: 4.0696  LR: 0.00000202  \n",
      "Epoch: [4][5400/5462] Elapsed 38m 20s (remain 0m 25s) Loss: 0.5842(0.5160) Grad: 2.7037  LR: 0.00000195  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.5159  time: 2326s\n",
      "Epoch 4 - Save Best Score: 0.5159 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][5461/5462] Elapsed 38m 46s (remain 0m 0s) Loss: 0.5176(0.5159) Grad: 5.6069  LR: 0.00000191  \n",
      "Epoch: [5][0/5462] Elapsed 0m 0s (remain 56m 31s) Loss: 0.5139(0.5139) Grad: 2.8062  LR: 0.00000191  \n",
      "Epoch: [5][100/5462] Elapsed 0m 43s (remain 38m 11s) Loss: 0.5017(0.4953) Grad: 1.9788  LR: 0.00000184  \n",
      "Epoch: [5][200/5462] Elapsed 1m 25s (remain 37m 24s) Loss: 0.6768(0.4991) Grad: 2.8250  LR: 0.00000178  \n",
      "Epoch: [5][300/5462] Elapsed 2m 8s (remain 36m 40s) Loss: 0.4986(0.4993) Grad: 2.3152  LR: 0.00000171  \n",
      "Epoch: [5][400/5462] Elapsed 2m 50s (remain 35m 58s) Loss: 0.5037(0.4978) Grad: 2.5982  LR: 0.00000165  \n",
      "Epoch: [5][500/5462] Elapsed 3m 33s (remain 35m 14s) Loss: 0.4984(0.4989) Grad: 2.5655  LR: 0.00000159  \n",
      "Epoch: [5][600/5462] Elapsed 4m 16s (remain 34m 31s) Loss: 0.4699(0.4989) Grad: 3.7783  LR: 0.00000153  \n",
      "Epoch: [5][700/5462] Elapsed 4m 58s (remain 33m 48s) Loss: 0.4690(0.4989) Grad: 3.3792  LR: 0.00000146  \n",
      "Epoch: [5][800/5462] Elapsed 5m 41s (remain 33m 5s) Loss: 0.4384(0.4994) Grad: 2.5931  LR: 0.00000141  \n",
      "Epoch: [5][900/5462] Elapsed 6m 23s (remain 32m 23s) Loss: 0.6810(0.5000) Grad: 6.8987  LR: 0.00000135  \n",
      "Epoch: [5][1000/5462] Elapsed 7m 6s (remain 31m 40s) Loss: 0.5549(0.5012) Grad: 3.5426  LR: 0.00000129  \n",
      "Epoch: [5][1100/5462] Elapsed 7m 49s (remain 30m 57s) Loss: 0.5509(0.5024) Grad: 2.5869  LR: 0.00000123  \n",
      "Epoch: [5][1200/5462] Elapsed 8m 31s (remain 30m 15s) Loss: 0.5832(0.5028) Grad: 4.1041  LR: 0.00000118  \n",
      "Epoch: [5][1300/5462] Elapsed 9m 14s (remain 29m 32s) Loss: 0.5026(0.5030) Grad: 11.1384  LR: 0.00000113  \n",
      "Epoch: [5][1400/5462] Elapsed 9m 56s (remain 28m 49s) Loss: 0.5415(0.5030) Grad: 2.8982  LR: 0.00000107  \n",
      "Epoch: [5][1500/5462] Elapsed 10m 39s (remain 28m 7s) Loss: 0.3781(0.5037) Grad: 2.2315  LR: 0.00000102  \n",
      "Epoch: [5][1600/5462] Elapsed 11m 22s (remain 27m 24s) Loss: 0.4345(0.5033) Grad: 2.2071  LR: 0.00000097  \n",
      "Epoch: [5][1700/5462] Elapsed 12m 4s (remain 26m 42s) Loss: 0.4051(0.5037) Grad: 2.8049  LR: 0.00000092  \n",
      "Epoch: [5][1800/5462] Elapsed 12m 47s (remain 25m 59s) Loss: 0.5527(0.5039) Grad: 3.3127  LR: 0.00000088  \n",
      "Epoch: [5][1900/5462] Elapsed 13m 29s (remain 25m 16s) Loss: 0.4963(0.5041) Grad: 2.4949  LR: 0.00000083  \n",
      "Epoch: [5][2000/5462] Elapsed 14m 12s (remain 24m 34s) Loss: 0.6278(0.5046) Grad: 2.8404  LR: 0.00000078  \n",
      "Epoch: [5][2100/5462] Elapsed 14m 55s (remain 23m 51s) Loss: 0.4991(0.5048) Grad: 2.1604  LR: 0.00000074  \n",
      "Epoch: [5][2200/5462] Elapsed 15m 37s (remain 23m 9s) Loss: 0.4793(0.5050) Grad: 3.0617  LR: 0.00000070  \n",
      "Epoch: [5][2300/5462] Elapsed 16m 20s (remain 22m 26s) Loss: 0.6107(0.5050) Grad: 3.6209  LR: 0.00000066  \n",
      "Epoch: [5][2400/5462] Elapsed 17m 2s (remain 21m 43s) Loss: 0.4486(0.5051) Grad: 2.9912  LR: 0.00000061  \n",
      "Epoch: [5][2500/5462] Elapsed 17m 45s (remain 21m 1s) Loss: 0.4109(0.5057) Grad: 3.0359  LR: 0.00000058  \n",
      "Epoch: [5][2600/5462] Elapsed 18m 27s (remain 20m 18s) Loss: 0.5736(0.5053) Grad: 4.0505  LR: 0.00000054  \n",
      "Epoch: [5][2700/5462] Elapsed 19m 10s (remain 19m 36s) Loss: 0.4842(0.5054) Grad: 4.3238  LR: 0.00000050  \n",
      "Epoch: [5][2800/5462] Elapsed 19m 53s (remain 18m 53s) Loss: 0.4392(0.5056) Grad: 4.4125  LR: 0.00000047  \n",
      "Epoch: [5][2900/5462] Elapsed 20m 35s (remain 18m 10s) Loss: 0.4862(0.5062) Grad: 2.9882  LR: 0.00000043  \n",
      "Epoch: [5][3000/5462] Elapsed 21m 18s (remain 17m 28s) Loss: 0.4649(0.5061) Grad: 2.6875  LR: 0.00000040  \n",
      "Epoch: [5][3100/5462] Elapsed 22m 0s (remain 16m 45s) Loss: 0.4668(0.5061) Grad: 3.2761  LR: 0.00000037  \n",
      "Epoch: [5][3200/5462] Elapsed 22m 43s (remain 16m 3s) Loss: 0.4524(0.5061) Grad: 2.6101  LR: 0.00000034  \n",
      "Epoch: [5][3300/5462] Elapsed 23m 26s (remain 15m 20s) Loss: 0.6126(0.5058) Grad: 2.4731  LR: 0.00000031  \n",
      "Epoch: [5][3400/5462] Elapsed 24m 8s (remain 14m 37s) Loss: 0.5552(0.5055) Grad: 3.6327  LR: 0.00000028  \n",
      "Epoch: [5][3500/5462] Elapsed 24m 51s (remain 13m 55s) Loss: 0.4682(0.5055) Grad: 2.6467  LR: 0.00000025  \n",
      "Epoch: [5][3600/5462] Elapsed 25m 33s (remain 13m 12s) Loss: 0.5253(0.5054) Grad: 3.6668  LR: 0.00000023  \n",
      "Epoch: [5][3700/5462] Elapsed 26m 16s (remain 12m 30s) Loss: 0.4643(0.5054) Grad: 3.0583  LR: 0.00000021  \n",
      "Epoch: [5][3800/5462] Elapsed 26m 59s (remain 11m 47s) Loss: 0.5591(0.5052) Grad: 2.6117  LR: 0.00000018  \n",
      "Epoch: [5][3900/5462] Elapsed 27m 41s (remain 11m 4s) Loss: 0.5836(0.5053) Grad: 5.2387  LR: 0.00000016  \n",
      "Epoch: [5][4000/5462] Elapsed 28m 24s (remain 10m 22s) Loss: 0.4954(0.5056) Grad: 3.0468  LR: 0.00000014  \n",
      "Epoch: [5][4100/5462] Elapsed 29m 7s (remain 9m 39s) Loss: 0.5005(0.5057) Grad: 2.3026  LR: 0.00000012  \n",
      "Epoch: [5][4200/5462] Elapsed 29m 49s (remain 8m 57s) Loss: 0.5510(0.5057) Grad: 2.5426  LR: 0.00000011  \n",
      "Epoch: [5][4300/5462] Elapsed 30m 32s (remain 8m 14s) Loss: 0.5971(0.5057) Grad: 4.6270  LR: 0.00000009  \n",
      "Epoch: [5][4400/5462] Elapsed 31m 14s (remain 7m 31s) Loss: 0.5554(0.5058) Grad: 3.7343  LR: 0.00000007  \n",
      "Epoch: [5][4500/5462] Elapsed 31m 57s (remain 6m 49s) Loss: 0.5998(0.5058) Grad: 4.5058  LR: 0.00000006  \n",
      "Epoch: [5][4600/5462] Elapsed 32m 39s (remain 6m 6s) Loss: 0.5103(0.5059) Grad: 4.2725  LR: 0.00000005  \n",
      "Epoch: [5][4700/5462] Elapsed 33m 22s (remain 5m 24s) Loss: 0.6065(0.5059) Grad: 2.6654  LR: 0.00000004  \n",
      "Epoch: [5][4800/5462] Elapsed 34m 5s (remain 4m 41s) Loss: 0.5436(0.5060) Grad: 2.7701  LR: 0.00000003  \n",
      "Epoch: [5][4900/5462] Elapsed 34m 51s (remain 3m 59s) Loss: 0.6320(0.5063) Grad: 2.9519  LR: 0.00000002  \n",
      "Epoch: [5][5000/5462] Elapsed 35m 34s (remain 3m 16s) Loss: 0.5123(0.5063) Grad: 2.6751  LR: 0.00000001  \n",
      "Epoch: [5][5100/5462] Elapsed 36m 16s (remain 2m 34s) Loss: 0.4470(0.5063) Grad: 3.3676  LR: 0.00000001  \n",
      "Epoch: [5][5200/5462] Elapsed 36m 59s (remain 1m 51s) Loss: 0.5502(0.5062) Grad: 3.8964  LR: 0.00000000  \n",
      "Epoch: [5][5300/5462] Elapsed 37m 42s (remain 1m 8s) Loss: 0.4354(0.5062) Grad: 3.5039  LR: 0.00000000  \n",
      "Epoch: [5][5400/5462] Elapsed 38m 24s (remain 0m 26s) Loss: 0.4668(0.5062) Grad: 3.0411  LR: 0.00000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.5062  time: 2331s\n",
      "Epoch 5 - Save Best Score: 0.5062 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][5461/5462] Elapsed 38m 50s (remain 0m 0s) Loss: 0.4820(0.5062) Grad: 3.7955  LR: 0.00000000  \n"
     ]
    }
   ],
   "source": [
    "model = Custom_Bert_Simple()\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\n",
    "fold = 0\n",
    "tr_data = df[df['fold']!=fold].reset_index(drop=True)\n",
    "va_data = df[df['fold']==fold].reset_index(drop=True)\n",
    "tr_dataset = TrainDataset(tr_data,tokenizer)\n",
    "va_dataset = TrainDataset(va_data,tokenizer)\n",
    "val_result = train_loop(fold, model,tr_dataset, va_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_b-cAuYKG3Bg"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
